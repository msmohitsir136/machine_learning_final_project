---
title: "Are you exercising correctly? Qualitative assessment of weight lifting exercises"
author: "Mohit Singh"
date: "30/12/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

####Background:

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves. In this project, my goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

####Goal:
The goal of this project is to utilize machine learning to predict which one of the 5 ways (the classe variable in the dataset) the barbell lift falls under.  A training and test dataset has been provided.

---
###Setup

First, will setup the environment and download the dataset into R.  I noticed that the data contained values of "#DIV/0!", which I converted to NA upon importing the data.

```{r, cache=TRUE, results='hide'}
library(caret)
library(doMC)
registerDoMC(cores = 8)

set.seed(786)
setwd("/home/mohit/data_science/machine_learning_final_project")
#Download data
trainurl = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(trainurl, destfile = "PMLtrain.csv", method = "curl")
testurl = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(testurl, destfile = "PMLtest.csv", method = "curl")

#Read Into Data Frame
Train = read.csv("PMLtrain.csv", na.strings = c('#DIV/0!', 'NA'))
Test = read.csv("PMLtest.csv", na.strings = c('#DIV/0!', 'NA'))
```

---

####Data Exploration

My next step was to explore the data, using summary and plotting a bunch of variables against eachother.  I cannot show all of the plots I used and all the output because of space constraints, but here is what I found:  (1) there are lots of missing data, some columns appear to only be populated at the end of the time window. (2) Some columns are all NA values, so those should be excluded (3) The num_window variable and classe have a 1:1 relationship so I maybe I can summarize the data by window. 


```{r, results='hide', echo=FALSE}
head(Train)
summary(Train)

table(Train$new_window, Train$classe)
table(Train$user_name, Train$classe)
table(Train$num_window, Train$classe)
```

---

####Pre Processing

The data exploration phase revealed two problems that I must deal with (1) missing values and (2) the high-dimensionality of the dataset.  To begin with, I decided to "compress" each window to just one row that represented the mean values for the window.  I initially tried KNN imputation for missing values, but after understanding the context of the data just compressing the data to one row per window seemed like a good approach to get rid of missing values and simplify the dataset.  I also decided to throw out all the columns that were mostly NA and see how the model performs.  As you will read below, the model performed well even after throwing out all of these variables.  

For dimensionality reduction, I initially tried PCA.  I read the paper associated with this data, and I found that the researchers used a feature selection technique called "CFS", or correlation based feature selection.  However, random forests have built -in feature selection, so it is possible that PCA and CFS are not necessary.  Nevertheless I wanted to try different options for learning purposes.

Below is the code I used to cleanup the training data, by: (1) removing NA and zero variance columns (2) summarized training data by window, and (3) partitioned the training data into an 80/20 train and validtion set, just so I could have another piece of data to assess model accuracy. 

```{r, warning=F, results='hide'}
#Find columns that have near zero variance
library(caret)
NotZeroVarIndex = nearZeroVar(Train, saveMetrics = T)$zeroVar == F
ColumnsToKeep = names(Train)[NotZeroVarIndex]

#Find columns that are mostly NAs
KeepCol = function(x){
  return(sum(is.na(x)) > 19000)}

NACols = names(Train)[sapply(Train, KeepCol)]

#Build List of Columsn To Keep
ColumnsToKeep = ColumnsToKeep[!(ColumnsToKeep %in% c('X', 'user_name', 'cvtd_timestamp', 'new_window', 
                                                     'raw_timestamp_part_1', 'raw_timestamp_part_2', 'classe', NACols))]

#Subset All Data To Exclude Offenders
Train = Train[, c(ColumnsToKeep, 'classe')]
Test = Test[, ColumnsToKeep]


#Summarize Training Data by num_window.  
MeanNA = function(x){
  return(mean(x, na.rm = T))}

MTrain = aggregate(. ~ num_window, data = Train, MeanNA)
MTrain$classe = as.factor(MTrain$classe)

#Split Test Set into Test & Validation
index = createDataPartition(MTrain$classe, p= .8, list = FALSE)
MTrain = MTrain[index, ]
MValidate = MTrain[-index, ]

#Alternate Version of Training Set Split Into X & Y
XTrain = MTrain[, 1:ncol(MTrain) - 1]
YTrain = MTrain$classe
```


**Feature Selection, using CFS**  

```{r, warning=F}
library(FSelector)
#See What Relevant Features Are - But Exclude Num Window
RelevantFeatures = cfs(classe ~. , MTrain[2:54])
RelevantFeatures
```

**Perform PCA**
```{r, results='hide', message=FALSE, warning=FALSE}
CSx = preProcess(XTrain, method = c('center', 'scale'))
CSXtrain = predict(CSx, XTrain)
PCAx = princomp(CSXtrain)
summary(PCAx)
```
I surpressed the otuput of summary(PCAx), but what it shows me is that the first 12 prinicpal components explain 80% of the vairance of the dataset.  If I wanted to use PCA here, these first 12 might be a good cutoff to reduce dimensionality. 

####Model Training

I used the caret package and 10-fold cross validation for model evaluation and parameter tuning.  There are three versions of the model I tested (1) Random Forest Using PCA, (2) Random Forest withtout any priori feature selection, and (3) Random Forest Using feature selection from CFS.

Configure 10-fold cross validation and Pre-processing
```{r}
TC = trainControl(method = 'cv', number = 10)
rfGrid = expand.grid(mtry = (1:15))
```

1. Train random forest with PCA using default parameters.  Note that caret centers and scales data before doing pca.
```{r, warning=FALSE, results='hide', cache=TRUE, message=FALSE}
PCAx = preProcess(XTrain, method = 'pca', thresh = .8)
XTrainPCA = predict(PCAx, XTrain)
rfPCA = train(x = XTrainPCA, y = YTrain, method = 'rf', trainControl = TC, tuneGrid = rfGrid)
```


2. Train Random Forest On Dataset "Out of the Box", (let the model work on its own)
```{r, warning=FALSE, results='hide', cache=TRUE, message=FALSE}
rf = train(classe~., data = MTrain, method = 'rf',
              trainControl = TC, tuneGrid = rfGrid, preProcess = c('center', 'scale'))
```

3. Train Random Forest With CFS Feature Selection and Center/Scaling

```{r, warning=FALSE, results='hide', cache=TRUE, message=FALSE}
rfCFS = train(classe~., data = MTrain[,c(RelevantFeatures, 'classe')], method = 'rf', preProcess = c('center', 'scale'), 
            trainControl = TC, tuneGrid = rfGrid)
```


##Model Selection

Now I am going to compare all of the models using some builtin caret functionality
```{r}
resamps <- resamples(list(rf.plain= rf,
                          rf.pca = rfPCA,
                          rf.cfs = rfCFS))
                          
trellis.par.set(caretTheme())
dotplot(resamps, main = 'Model Comparison: Random Forest With and Without Feature Selection', ylab = 'Model Used')
```

**Conclusion:** It was best to use random forest "out of the box" (which is rf.plain in the above graph), compared to trying to do PCA or CFS first to further reduce dimensionality.  This is probably because random forests have built in feature selection.  Therefore, I am going to proceed with the rf model as the performance looks pretty good.   

---

##Model Tuning
I want to make sure parameter tuning occured optimally, and that my arbritary choosen tuning grid of 1:15 seems to be ok. I initially used the default tuning 
grid provided by caret, but after looking at the intial resuls decided to implement my own tuning grid of 1:15 trees.  

Inspect Tuning Paremeters:
```{r}


library(ggplot2)
library(reshape2)
results = data.frame('mtry' = rfPCA$results$mtry, 'rfPCA' = rfPCA$results$Accuracy, 'rfCFS' = rfCFS$results$Accuracy, 'rf' = rf$results$Accuracy)
Mresults = melt(results, id.var = 'mtry')

#Get The Optimal Tuning Parameters Choosen By Caret
OptimalParams = Mresults[(Mresults$variable == 'rf' & Mresults$mtry == rf$bestTune[1,1]) | (Mresults$variable == 'rfPCA' & Mresults$mtry == rfPCA$bestTune[1,1]) |
(Mresults$variable == 'rfCFS' & Mresults$mtry == rfCFS$bestTune[1,1]), ]
names(OptimalParams) = c('mtry', 'Model.Type', 'Accuracy')
#Graph Everything
names(Mresults) = c('mtry', 'Model.Type', 'Accuracy')
ggplot(data=Mresults, aes(x=mtry, y=Accuracy, group=Model.Type, color = Model.Type)) + geom_line() + ggtitle('Random Forest Models Compared -Tuning Parameters\n Dots Are Optimal Tuning Paremeters Choosen by Caret') + geom_point(data = OptimalParams, size = 5, label = OptimalParams$mtry) 
```


The tuning parameters choosen by caret seem to be reasonable represented by the dots in the above graph. 
I feel comfortable that 1-15 trees were a sufficient tuning range for this problem, given the shape of the curves here.  I also am still confident that the "plain rf"
 is the best model here and that the tuning paremeters didn't bias me in the wrong way.  

---

###Model Evaluation
Now I want to estimate how the model will perform on a new dataset.  A reasonable estimate of this is the **mean accuracy returned by 10-fold cross validation, which was approximately 87.9% Accuracy. **  However, I also held out a validation set by partitioning the test set into an 80/20 split in the beginning of this exercise.  I did this with hopes of getting a more accurate estimate by testing my model against a dataset that it has never seen before.  

However, **when I ran my model against the validation set, I achieved 100% accuracy!**   Even though the mean accuracy score from 10 fold CV was 87.9%, caret retrains the model over the entire training set once it finds its optimal tuning parameters.  So it is possible that the random forest became more accurate after seeing all the data in the training set.  However, I don't want to bank on my model achieving 100% accuracy and **am going to stay with a more conservative estimate of 87.9% obtained from 10-fold cross validation.**  

Here is a readout of the final model's result on cross validation:

```{r}
rf$finalModel
rf$bestTune
rf$results
```


Below is the confusion matrix for my model run against the validation dataset. 
```{r, warning=FALSE, message=FALSE}
confusionMatrix(predict(rf, MValidate), MValidate$classe)
```
Again, I think this is optimistic or maybe I just got lucky here on the validation set.  Therefore, I think its wise to stick with 87.9% accuracy. 

###Conclusion
Here are the main takeaways from this study:

- Pre processing was a very important step to clean up the data:  removing NA and Zero Variance Columns
- Summarizing the data accross window simplified the analysis and further allowed me reduce dimensioanlity by getting rid of timestamps
- Random Forest was able to handle high dimensionality on its own without any dimensionality reduction techniques
- Was able to achieve a cross validation mean accuracy of 87.9%, which is a reasonable estimate of how the model will perform on new data, especially given that the accuracy on the hold-out data was 100%.  

